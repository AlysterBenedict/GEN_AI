{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "367a7b86",
   "metadata": {},
   "source": [
    "Program 4:\n",
    "Use word embeddings to improve prompts for Generative AI model. Retrieve similar words using word embeddings. Use the similar words to enrich a GenAI prompt. Use the AI model to generate responses for the original and enriched prompts. Compare the outputs in terms of detail and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8928e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: transformers in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from transformers) (0.31.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bened\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: click in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bened\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bened\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\bened\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bened\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bened\\anaconda3\\envs\\transformers_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bened\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\bened\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained word vectors...\n",
      "\n",
      "Loading GPT-2 model...\n",
      "WARNING:tensorflow:From c:\\Users\\bened\\anaconda3\\envs\\transformers_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\bened\\anaconda3\\envs\\transformers_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bened\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Original Prompt: Who is king.\n",
      "Replacing 'king' â†’ 'prince'\n",
      "\n",
      "ðŸ”¹ Enriched Prompt: Who is prince .\n",
      "\n",
      "Generating response for the original prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Prompt Response:\n",
      "Who is king.\n",
      "\n",
      "1 The king is the king.\n",
      "\n",
      "2 The king is the king.\n",
      "\n",
      "3 The king is the king.\n",
      "\n",
      "4 The king is the king.\n",
      "\n",
      "5 The king is the king.\n",
      "\n",
      "6 The king is the king.\n",
      "\n",
      "7 The king is the king.\n",
      "\n",
      "8 The king is the king.\n",
      "\n",
      "9 The king is the king.\n",
      "\n",
      "10 The king is the king.\n",
      "\n",
      "11 The king is the king.\n",
      "\n",
      "12 The king is the king.\n",
      "\n",
      "13 The king is the king.\n",
      "\n",
      "14 The king is the king.\n",
      "\n",
      "15 The king is the king.\n",
      "\n",
      "16 The king is the king.\n",
      "\n",
      "17 The king is the king.\n",
      "\n",
      "18 The king is the king.\n",
      "\n",
      "19 The king is the king.\n",
      "\n",
      "20 The king is the king.\n",
      "\n",
      "21 The king is the king.\n",
      "\n",
      "22 The king is the king.\n",
      "\n",
      "23 The king is the king.\n",
      "\n",
      "24 The king is the king.\n",
      "\n",
      "25 The king is the king.\n",
      "\n",
      "26 The king is the king.\n",
      "\n",
      "27 The king is the king.\n",
      "\n",
      "28 The king is the king.\n",
      "\n",
      "29 The\n",
      "\n",
      "Generating response for the enriched prompt...\n",
      "\n",
      "Enriched Prompt Response:\n",
      "Who is prince ...?\" he asked. \"My lord,\" said Lord Robert. And that was all the prince, for he had been told that all the lords had been told that Robert was the prince of the kingdom. And it went on in all its full length, till at last Sir David came to him to his senses, and told him that he was lord of the kingdom, and that he had been told that he was the prince of the kingdom, and that he was lord of the people, and that he was lord of the whole house of Israel. And they said to him, \"Your lords, be now with us,\" and he did; and they went on into the city, and there he found Sir David, and found all the people and all the people of the people of God, and all the people of Israel, and all the people of the world and all the world, and all the people of God, and all the world and all the world and all the world, and all the world and all the world, and all the world and all the world, and all the world and all the world, and all the world and all the world, and all the world and all the world, and all the world and all the world, and all the\n",
      "\n",
      "ðŸ“Š Comparison of Responses:\n",
      "Original Prompt Response Length: 739\n",
      "Enriched Prompt Response Length: 1060\n",
      "Original Prompt Sentences: 29\n",
      "Enriched Prompt Sentences: 7\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "! pip install gensim transformers nltk matplotlib\n",
    "\n",
    "# Import libraries\n",
    "import gensim.downloader as api\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')  # Correct resource for tokenization\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load pre-trained word vectors\n",
    "print(\"Loading pre-trained word vectors...\")\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")  # You can change to another model if desired\n",
    "\n",
    "# Function to replace a keyword in the prompt with its most similar word\n",
    "def replace_keyword_in_prompt(prompt, keyword, word_vectors, topn=1):\n",
    "    words = word_tokenize(prompt)\n",
    "    enriched_words = []\n",
    "    for word in words:\n",
    "        cleaned_word = word.lower().strip(string.punctuation)\n",
    "        if cleaned_word == keyword.lower():\n",
    "            try:\n",
    "                similar_words = word_vectors.most_similar(cleaned_word, topn=topn)\n",
    "                if similar_words:\n",
    "                    replacement_word = similar_words[0][0]\n",
    "                    print(f\"Replacing '{word}' â†’ '{replacement_word}'\")\n",
    "                    enriched_words.append(replacement_word)\n",
    "                    continue  # Skip appending original\n",
    "            except KeyError:\n",
    "                print(f\"'{keyword}' not found in the vocabulary. Using original word.\")\n",
    "        enriched_words.append(word)\n",
    "    enriched_prompt = \" \".join(enriched_words)\n",
    "    print(f\"\\nðŸ”¹ Enriched Prompt: {enriched_prompt}\")\n",
    "    return enriched_prompt\n",
    "\n",
    "# Load GPT-2 model for text generation\n",
    "print(\"\\nLoading GPT-2 model...\")\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Function to generate response using GPT-2\n",
    "def generate_response(prompt, max_length=100):\n",
    "    try:\n",
    "        response = generator(prompt, max_length=max_length, num_return_sequences=1)\n",
    "        return response[0]['generated_text']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example original prompt\n",
    "original_prompt = \"Who is king.\"\n",
    "print(f\"\\nðŸ”¹ Original Prompt: {original_prompt}\")\n",
    "\n",
    "# Define the keyword to be enriched\n",
    "key_term = \"king\"\n",
    "\n",
    "# Enrich the prompt\n",
    "enriched_prompt = replace_keyword_in_prompt(original_prompt, key_term, word_vectors)\n",
    "\n",
    "# Generate responses\n",
    "print(\"\\nGenerating response for the original prompt...\")\n",
    "original_response = generate_response(original_prompt)\n",
    "print(\"\\nOriginal Prompt Response:\")\n",
    "print(original_response)\n",
    "\n",
    "print(\"\\nGenerating response for the enriched prompt...\")\n",
    "enriched_response = generate_response(enriched_prompt)\n",
    "print(\"\\nEnriched Prompt Response:\")\n",
    "print(enriched_response)\n",
    "\n",
    "# Comparison\n",
    "print(\"\\nðŸ“Š Comparison of Responses:\")\n",
    "print(\"Original Prompt Response Length:\", len(original_response))\n",
    "print(\"Enriched Prompt Response Length:\", len(enriched_response))\n",
    "print(\"Original Prompt Sentences:\", original_response.count('.'))\n",
    "print(\"Enriched Prompt Sentences:\", enriched_response.count('.'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
